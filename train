import time
# from kan import KAN
from efficient_kan import KAN
import numpy as np
import matplotlib.pyplot as plt
import copy
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import random
import math


class MyDataset:
    def __init__(self):
        super().__init__()

    def train_set(self):
        train_data = np.fromfile("train_data.dat", dtype=float, sep=",").reshape((-1, 10, 20, 2)).tolist()
        train_label = np.fromfile("train_label.dat", dtype=float, sep=",").reshape((-1, 10, 20, 2)).tolist()
        return TensorDataset(torch.tensor(train_data), torch.tensor(train_label))

    def test_set(self):
        test_data = np.fromfile("test_data.dat", dtype=float, sep=",").reshape((-1, 10, 20, 2)).tolist()
        test_label = np.fromfile("test_label.dat", dtype=float, sep=",").reshape((-1, 10, 20, 2)).tolist()
        return TensorDataset(torch.tensor(test_data), torch.tensor(test_label))


def adjust_learning_rate(optimizer):
    lr = optimizer.param_groups[0]['lr'] * 10
    return lr


class GRKAN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GRKAN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.W = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size * 3))
        self.bias = nn.Parameter(torch.Tensor(hidden_size * 3))
        self.init_weights()

        self.kan1 = KAN([input_size + hidden_size, hidden_size], base_activation=nn.Tanh())
        self.kan2 = KAN([hidden_size + hidden_size, hidden_size], base_activation=nn.Tanh())

    def init_weights(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)

    def forward(self, input):
        batch_size = input.size(0)
        seq_len = input.size(1)
        hidden_state = torch.zeros(batch_size, self.hidden_size, dtype=torch.float32).to('cuda') 
        cell_state = torch.zeros(batch_size, self.hidden_size, dtype=torch.float32).to('cuda')
        outputs = []
        HS = self.hidden_size
        for i in range(seq_len):
            combined = torch.cat((input[:, i, :], hidden_state), dim=1)
            gates = combined @ self.W + self.bias
            z_t, r_t, f_t = (
                torch.sigmoid(gates[:, :HS]),  # upset
                torch.sigmoid(gates[:, HS:HS * 2]),  # reset
                torch.sigmoid(gates[:, HS * 2:]),  # forget
            )

            combined = torch.cat((input[:, i, :], r_t * hidden_state), dim=1)
            c_hat_t = self.kan1(combined)
            cell_state = self.kan2(torch.cat((c_hat_t, f_t * cell_state), dim=1)) 
            hidden_state = (1-z_t) * hidden_state + z_t * cell_state 
            outputs.append(hidden_state.unsqueeze(1)) 
        outputs = torch.cat(outputs, dim=1)  
        return outputs, hidden_state  


class GateRecurrentKan(nn.Module):
    def __init__(self, dim, num_layer):
        super().__init__()

        f1 = nn.Linear(40, dim[0] // 2).to('cuda')
        f2 = nn.Linear(dim[0] // 2, dim[0]).to('cuda')
        torch.nn.init.xavier_normal_(f1.weight.data)
        torch.nn.init.xavier_normal_(f2.weight.data)
        self.input_proj = nn.Sequential(f1, nn.Tanh(), f2, nn.Tanh())
        self.Norm_1 = nn.LayerNorm([10, dim[0]])
        self.Norm_2 = nn.LayerNorm([10, dim[1]])
        self.Norm_3 = nn.LayerNorm(dim[1])

        self.grukan_1 = GRKAN(dim[0], dim[0]).to('cuda')
        self.grukan_2 = GRKAN(dim[0], dim[1]).to('cuda')

        fnn1 = nn.Linear(dim[1], dim[1] // 2).to('cuda')
        torch.nn.init.xavier_normal_(fnn1.weight.data)
        fnn2 = nn.Linear(dim[1] // 2, 40).to('cuda')
        torch.nn.init.xavier_normal_(fnn2.weight.data)
        self.out_proj = nn.Sequential(fnn1, nn.Tanh(), fnn2, nn.Tanh())


    def forward(self, x):
        b, t, w, c = x.size()
        x = x.view(b, t, w*c)
        x = self.input_proj(x)  # (b, t, h, c)
        x = self.Norm_1(x)

        x, h = self.grukan_1(x)
        x = self.Norm_2(x)
        x, h = self.grukan_2(x)
        h = self.Norm_3(h)

        x = self.out_proj(h).view(b, w, c)
        return x


def model_fit():
    M = MyDataset()
    train_data = M.train_set()
    test_data = M.test_set()

    train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=0)
    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=0)

    mymodule = GateRecurrentKan(dim=[64, 64], num_layer=2)
    mymodule = mymodule.to(device) 

    loss = torch.nn.MSELoss()
    learn_step = 0.001
    optim = torch.optim.Adam(mymodule.parameters(), lr=learn_step)
    epoch = 4000
    train_loss = []
    test_loss = []

    for i in range(epoch):
        mymodule.train() 
        epoch_loss = 0
        train_step = 0

        for data in train_dataloader:
            encoder_data, targets = data
            encoder_data = encoder_data.to(device)
            targets = targets.to(device).unsqueeze(1).repeat(1, 20, 1)
            outputs = mymodule(encoder_data)
            result_loss = loss(outputs.to(torch.float32), targets.to(torch.float32))

            optim.zero_grad()
            result_loss.backward()
            optim.step()

            train_step += 1
            epoch_loss += result_loss.item()

        train_loss.append(epoch_loss / train_step)
        mymodule.eval()  
        val_loss = 0
        tt = 0
        with torch.no_grad(): 
            for test_data in test_dataloader:
                tt += 1
                encoder_data, label = test_data
                encoder_data = encoder_data.to(device)
                label = label.to(device).unsqueeze(1).repeat(1, 20, 1)
                outputs_ = mymodule(encoder_data)
                test_result_loss = loss(outputs_, label)
                val_loss += test_result_loss.item()
            test_loss.append(val_loss / tt)

        if (i + 1) % 50 == 0 or i == 1:
            torch.save(mymodule.state_dict(), "train_model_efficient/mymodule_{}.pth".format((i + 1)))
            print("{}th Loss is {}".format((i + 1), test_result_loss.item()))
    loss_value = np.array(train_loss)
    loss_value.tofile("train_loss_efficient/loss.dat", sep=",", format="%f")
    val_loss_value = np.array(test_loss)
    val_loss_value.tofile("train_loss_efficient/val_loss.dat", sep=",", format="%f")
    k = [i - 0.5 for i in range(len(loss_value))]

    plt.figure(1)
    plt.plot(k, loss_value)
    plt.plot(k, val_loss_value)
    plt.title('Model loss')
    plt.ylabel('loss')
    plt.xlabel('Epoch')
    plt.legend('Train Loss', loc='upper right')
    plt.show()
    return


def model_predict():
    test_data = MyDataset().test_set()
    n = 1
    test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)

    mymodule = GateRecurrentKan(dim=[64, 64], num_layer=2)
    mymodule = mymodule.to(device) 
    mymodule.load_state_dict(torch.load("mymodule_{}.pth".format(3850)))

    mymodule.eval()
    out_all = []
    label = []
    encoder_input = []
    decoder_input = []
    det_t = 0
    param_size = 0

    with torch.no_grad():
        for data in test_dataloader:
            encoder_data, targets = data
            encoder_data = encoder_data.to(device)
           
            outputs = mymodule(encoder_data)
            out_all.append(outputs[0].tolist())
            label.append(targets[0].tolist())
            encoder_input.append(encoder_data[0].tolist())
           
    out_all = np.array(out_all)
    label = np.array(label) 
   
    out_mean = np.mean(out_all, 1)
    
    out_mean = out_mean.reshape((n, -1, 2))
    label = label.reshape((n, -1, 2))
    det = out_mean - label
    error = np.sqrt(np.power(det[:, :, 0], 2) + np.power(det[:, :, 1], 2))
    rmse = np.mean(error, 0)
    plt.figure(2)
    plt.plot(rmse)
   
    plt.show()
    return


if __name__ == '__main__':
    # model_fit()
    model_predict()
